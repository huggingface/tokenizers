import _typeshed
import tokenizers
import tokenizers.decoders
import tokenizers.models
import tokenizers.normalizers
import tokenizers.pre_tokenizers
import tokenizers.processors
import tokenizers.trainers
import typing

__version__: typing.Final[str]

class AddedToken:
    def __eq__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __ge__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __getstate__(self, /) -> typing.Any: ...
    def __gt__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __hash__(self, /) -> int: ...
    def __le__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __lt__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __ne__(self, /, other: tokenizers.AddedToken) -> bool: ...
    def __new__(cls, /, content: str | None = None, **kwargs) -> None: ...
    def __repr__(self, /) -> str: ...
    def __setstate__(self, /, state: typing.Any) -> typing.Any: ...
    def __str__(self, /) -> str: ...
    @property
    def content(self, /) -> str: ...
    @content.setter
    def content(self, /, content: str) -> None: ...
    @property
    def lstrip(self, /) -> bool: ...
    @property
    def normalized(self, /) -> bool: ...
    @property
    def rstrip(self, /) -> bool: ...
    @property
    def single_word(self, /) -> bool: ...
    @property
    def special(self, /) -> bool: ...
    @special.setter
    def special(self, /, special: bool) -> None: ...

class Encoding:
    def __getstate__(self, /) -> typing.Any: ...
    def __len__(self, /) -> int: ...
    def __new__(cls, /) -> None: ...
    def __repr__(self, /) -> str: ...
    def __setstate__(self, /, state: typing.Any) -> typing.Any: ...
    @property
    def attention_mask(self, /) -> typing.Any: ...
    def char_to_token(self, /, char_pos: int, sequence_index: int = 0) -> typing.Any: ...
    def char_to_word(self, /, char_pos: int, sequence_index: int = 0) -> typing.Any: ...
    @property
    def ids(self, /) -> typing.Any: ...
    @staticmethod
    def merge(encodings: typing.Any, growing_offsets: bool = True) -> Encoding: ...
    @property
    def n_sequences(self, /) -> int: ...
    @property
    def offsets(self, /) -> typing.Any: ...
    @property
    def overflowing(self, /) -> typing.Any: ...
    def pad(self, /, length: int, **kwargs) -> None: ...
    @property
    def sequence_ids(self, /) -> typing.Any: ...
    def set_sequence_id(self, /, sequence_id: int) -> None: ...
    @property
    def special_tokens_mask(self, /) -> typing.Any: ...
    def token_to_chars(self, /, token_index: int) -> typing.Any: ...
    def token_to_sequence(self, /, token_index: int) -> typing.Any: ...
    def token_to_word(self, /, token_index: int) -> typing.Any: ...
    @property
    def tokens(self, /) -> typing.Any: ...
    def truncate(self, /, max_length: int, stride: int = 0, direction: str = "right") -> None: ...
    @property
    def type_ids(self, /) -> typing.Any: ...
    @property
    def word_ids(self, /) -> typing.Any: ...
    def word_to_chars(self, /, word_index: int, sequence_index: int = 0) -> typing.Any: ...
    def word_to_tokens(self, /, word_index: int, sequence_index: int = 0) -> typing.Any: ...
    @property
    def words(self, /) -> typing.Any: ...

class NormalizedString:
    def __getitem__(self, /, range: int | tuple[int, int] | typing.Any) -> typing.Any: ...
    def __new__(cls, /, sequence: str) -> None: ...
    def __repr__(self, /) -> str: ...
    def __str__(self, /) -> str: ...
    def append(self, /, s: str) -> None: ...
    def clear(self, /) -> None: ...
    def filter(self, /, func: typing.Any) -> typing.Any: ...
    def for_each(self, /, func: typing.Any) -> typing.Any: ...
    def lowercase(self, /) -> None: ...
    def lstrip(self, /) -> None: ...
    def map(self, /, func: typing.Any) -> typing.Any: ...
    def nfc(self, /) -> None: ...
    def nfd(self, /) -> None: ...
    def nfkc(self, /) -> None: ...
    def nfkd(self, /) -> None: ...
    @property
    def normalized(self, /) -> str: ...
    @property
    def original(self, /) -> str: ...
    def prepend(self, /, s: str) -> None: ...
    def replace(self, /, pattern: str | tokenizers.Regex, content: str) -> typing.Any: ...
    def rstrip(self, /) -> None: ...
    def slice(self, /, range: int | tuple[int, int] | typing.Any) -> typing.Any: ...
    def split(self, /, pattern: str | tokenizers.Regex, behavior: typing.Any) -> typing.Any: ...
    def strip(self, /) -> None: ...
    def uppercase(self, /) -> None: ...

class PreTokenizedString:
    def __new__(cls, /, s: str) -> None: ...
    def get_splits(self, /, offset_referential: typing.Any = ..., offset_type: typing.Any = ...) -> typing.Any: ...
    def normalize(self, /, func: typing.Any) -> typing.Any: ...
    def split(self, /, func: typing.Any) -> typing.Any: ...
    def to_encoding(self, /, type_id: int = 0, word_idx: int | None = None) -> Encoding: ...
    def tokenize(self, /, func: typing.Any) -> typing.Any: ...

class Regex:
    def __new__(cls, /, s: str) -> None: ...

class Token:
    def __new__(cls, /, id: int, value: str, offsets: typing.Any) -> None: ...
    def as_tuple(self, /) -> typing.Any: ...
    @property
    def id(self, /) -> int: ...
    @property
    def offsets(self, /) -> typing.Any: ...
    @property
    def value(self, /) -> str: ...

class Tokenizer:
    def __getnewargs__(self, /) -> typing.Any: ...
    def __getstate__(self, /) -> typing.Any: ...
    def __new__(cls, /, model: tokenizers.models.Model) -> None: ...
    def __repr__(self, /) -> str: ...
    def __setstate__(self, /, state: typing.Any) -> typing.Any: ...
    def __str__(self, /) -> str: ...
    def add_special_tokens(self, /, tokens: typing.Any) -> int: ...
    def add_tokens(self, /, tokens: typing.Any) -> int: ...
    def async_decode_batch(self, /, sequences: typing.Any, skip_special_tokens: bool = True) -> typing.Any: ...
    def async_encode(
        self,
        /,
        sequence: typing.Any,
        pair: typing.Any | None = None,
        is_pretokenized: bool = False,
        add_special_tokens: bool = True,
    ) -> typing.Any: ...
    def async_encode_batch(
        self, /, input: typing.Any, is_pretokenized: bool = False, add_special_tokens: bool = True
    ) -> typing.Any: ...
    def async_encode_batch_fast(
        self, /, input: typing.Any, is_pretokenized: bool = False, add_special_tokens: bool = True
    ) -> typing.Any: ...
    def decode(self, /, ids: typing.Any, skip_special_tokens: bool = True) -> str: ...
    def decode_batch(self, /, sequences: typing.Any, skip_special_tokens: bool = True) -> list[str]: ...
    @property
    def decoder(self, /) -> typing.Any: ...
    @decoder.setter
    def decoder(self, /, decoder: tokenizers.decoders.Decoder | None) -> None: ...
    def enable_padding(self, /, **kwargs) -> None: ...
    def enable_truncation(self, /, max_length: int, **kwargs) -> None: ...
    def encode(
        self,
        /,
        sequence: typing.Any,
        pair: typing.Any | None = None,
        is_pretokenized: bool = False,
        add_special_tokens: bool = True,
    ) -> Encoding: ...
    def encode_batch(
        self, /, input: typing.Any, is_pretokenized: bool = False, add_special_tokens: bool = True
    ) -> list[Encoding]: ...
    def encode_batch_fast(
        self, /, input: typing.Any, is_pretokenized: bool = False, add_special_tokens: bool = True
    ) -> list[Encoding]: ...
    @property
    def encode_special_tokens(self, /) -> bool: ...
    @encode_special_tokens.setter
    def encode_special_tokens(self, /, value: bool) -> None: ...
    @staticmethod
    def from_buffer(buffer: typing.Any) -> Tokenizer: ...
    @staticmethod
    def from_file(path: str) -> Tokenizer: ...
    @staticmethod
    def from_pretrained(identifier: str, revision: str = ..., token: str | None = None) -> Tokenizer: ...
    @staticmethod
    def from_str(json: str) -> Tokenizer: ...
    def get_added_tokens_decoder(self, /) -> dict[int, AddedToken]: ...
    def get_vocab(self, /, with_added_tokens: bool = True) -> dict[str, int]: ...
    def get_vocab_size(self, /, with_added_tokens: bool = True) -> int: ...
    def id_to_token(self, /, id: int) -> str | None: ...
    @property
    def model(self, /) -> typing.Any: ...
    @model.setter
    def model(self, /, model: tokenizers.models.Model) -> None: ...
    def no_padding(self, /) -> None: ...
    def no_truncation(self, /) -> None: ...
    @property
    def normalizer(self, /) -> typing.Any: ...
    @normalizer.setter
    def normalizer(self, /, normalizer: tokenizers.normalizers.Normalizer | None) -> None: ...
    def num_special_tokens_to_add(self, /, is_pair: bool) -> int: ...
    @property
    def padding(self, /) -> typing.Any: ...
    def post_process(
        self,
        /,
        encoding: tokenizers.Encoding,
        pair: tokenizers.Encoding | None = None,
        add_special_tokens: bool = True,
    ) -> tokenizers.Encoding: ...
    @property
    def post_processor(self, /) -> typing.Any: ...
    @post_processor.setter
    def post_processor(self, /, processor: tokenizers.processors.PostProcessor | None) -> None: ...
    @property
    def pre_tokenizer(self, /) -> typing.Any: ...
    @pre_tokenizer.setter
    def pre_tokenizer(self, /, pretok: tokenizers.pre_tokenizers.PreTokenizer | None) -> None: ...
    def save(self, /, path: str, pretty: bool = True) -> None: ...
    def to_str(self, /, pretty: bool = False) -> str: ...
    def token_to_id(self, /, token: str) -> int | None: ...
    def train(self, /, files: typing.Any, trainer: tokenizers.trainers.Trainer | None = None) -> typing.Any: ...
    def train_from_iterator(
        self, /, iterator: typing.Any, trainer: tokenizers.trainers.Trainer | None = None, length: int | None = None
    ) -> typing.Any: ...
    @property
    def truncation(self, /) -> typing.Any: ...

def __getattr__(name: str) -> _typeshed.Incomplete: ...
