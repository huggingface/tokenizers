import typing

class BPE:
    def __new__(cls, /, vocab: typing.Any | str | None = None, merges: typing.Any | str | None = None, **kwargs) -> None: ...
    def _clear_cache(self, /) -> typing.Any: ...
    def _resize_cache(self, /, capacity: int) -> typing.Any: ...
    @property
    def byte_fallback(self, /) -> bool: ...
    @byte_fallback.setter
    def byte_fallback(self, /, byte_fallback: bool) -> None: ...
    @property
    def continuing_subword_prefix(self, /) -> typing.Any: ...
    @continuing_subword_prefix.setter
    def continuing_subword_prefix(self, /, continuing_subword_prefix: str | None) -> None: ...
    @property
    def dropout(self, /) -> typing.Any: ...
    @dropout.setter
    def dropout(self, /, dropout: float | None) -> None: ...
    @property
    def end_of_word_suffix(self, /) -> typing.Any: ...
    @end_of_word_suffix.setter
    def end_of_word_suffix(self, /, end_of_word_suffix: str | None) -> None: ...
    @classmethod
    def from_file(cls, /, vocab: str, merges: str, **kwargs) -> BPE: ...
    @property
    def fuse_unk(self, /) -> bool: ...
    @fuse_unk.setter
    def fuse_unk(self, /, fuse_unk: bool) -> None: ...
    @property
    def ignore_merges(self, /) -> bool: ...
    @ignore_merges.setter
    def ignore_merges(self, /, ignore_merges: bool) -> None: ...
    @staticmethod
    def read_file(vocab: str, merges: str) -> typing.Any: ...
    @property
    def unk_token(self, /) -> typing.Any: ...
    @unk_token.setter
    def unk_token(self, /, unk_token: str | None) -> None: ...

class Model:
    def __getstate__(self, /) -> typing.Any: ...
    def __new__(cls, /) -> None: ...
    def __repr__(self, /) -> str: ...
    def __setstate__(self, /, state: typing.Any) -> typing.Any: ...
    def __str__(self, /) -> str: ...
    def get_trainer(self, /) -> typing.Any: ...
    def id_to_token(self, /, id: int) -> typing.Any: ...
    def save(self, /, folder: str, prefix: str | None = None, name: str | None = None) -> typing.Any: ...
    def token_to_id(self, /, token: str) -> typing.Any: ...
    def tokenize(self, /, sequence: str) -> typing.Any: ...

class Unigram:
    def __new__(cls, /, vocab: typing.Any | None = None, unk_id: int | None = None, byte_fallback: bool | None = None) -> None: ...
    def _clear_cache(self, /) -> typing.Any: ...
    def _resize_cache(self, /, capacity: int) -> typing.Any: ...

class WordLevel:
    def __new__(cls, /, vocab: typing.Any | str | None = None, unk_token: str | None = None) -> None: ...
    @classmethod
    def from_file(cls, /, vocab: str, unk_token: str | None = None) -> WordLevel: ...
    @staticmethod
    def read_file(vocab: str) -> typing.Any: ...
    @property
    def unk_token(self, /) -> str: ...
    @unk_token.setter
    def unk_token(self, /, unk_token: str) -> None: ...

class WordPiece:
    def __new__(cls, /, vocab: typing.Any | str | None = None, **kwargs) -> None: ...
    @property
    def continuing_subword_prefix(self, /) -> str: ...
    @continuing_subword_prefix.setter
    def continuing_subword_prefix(self, /, continuing_subword_prefix: str) -> None: ...
    @classmethod
    def from_file(cls, /, vocab: str, **kwargs) -> WordPiece: ...
    @property
    def max_input_chars_per_word(self, /) -> int: ...
    @max_input_chars_per_word.setter
    def max_input_chars_per_word(self, /, max: int) -> None: ...
    @staticmethod
    def read_file(vocab: str) -> typing.Any: ...
    @property
    def unk_token(self, /) -> str: ...
    @unk_token.setter
    def unk_token(self, /, unk_token: str) -> None: ...
