{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a70ba8-5cca-41ac-bd4b-4523e170ee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sander/Desktop/tokenizers/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json, gzip\n",
    "\n",
    "corpus_name=\"eng_latn_300mb\"\n",
    "dataset = load_dataset(\n",
    "                \"sanderland/monolingual-tokenizer-data\",\n",
    "                data_files=[f\"{corpus_name}.txt\"],\n",
    "                split=\"train\",\n",
    "                streaming=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b4c0d-246e-4823-9a92-1f4d9d6a3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokfile = f\"{corpus_name}-hf.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bb392d-36b9-42a7-aaff-866eb4bb1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFC, Lowercase, Sequence\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<|endoftext|>\"))\n",
    "tokenizer.normalizer = Sequence([NFC()])\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=50257, special_tokens=[\"<|endoftext|>\"], initial_alphabet=pre_tokenizers.ByteLevel.alphabet() )\n",
    "def get_training_corpus():\n",
    "    for row in dataset:\n",
    "        yield row[\"text\"]\n",
    "\n",
    "# 4. Train the tokenizer using the new iterator\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "tokenizer.save(tokfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885e06e1-1ef1-40cc-a1fc-cc1d0e281e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['Hello', ',', 'Ġthis', 'Ġis', 'Ġa', 'Ġtest', 'Ġof', 'Ġthe', 'Ġnew', 'ĠGP', 'T', '-', '2', 'Ġstyle', 'Ġtoken', 'izer', '.']\n",
      "Encoded IDs: [10452, 12, 417, 315, 258, 1343, 289, 263, 617, 8929, 52, 13, 18, 2493, 17741, 6208, 14]\n",
      "\n",
      "Encoded complex string: ['ĠĠĠ', 'ĠMultiple', 'Ġspaces', 'Ġand', 'Ġspecial', 'Ġcharacters', 'Ġlike', \"Ġ'\", 'Ã©', \"'\", '!']\n",
      "Encoded complex IDs: [44242, 17800, 6320, 286, 1439, 4077, 584, 1111, 2849, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(tokfile)\n",
    "\n",
    "encoded = tokenizer.encode(\"Hello, this is a test of the new GPT-2 style tokenizer.\")\n",
    "print(\"Encoded string:\", encoded.tokens)\n",
    "print(\"Encoded IDs:\", encoded.ids)\n",
    "\n",
    "encoded_complex = tokenizer.encode(\"    Multiple spaces and special characters like 'é'!\")\n",
    "print(\"\\nEncoded complex string:\", encoded_complex.tokens)\n",
    "print(\"Encoded complex IDs:\", encoded_complex.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2799286-beed-4362-a080-25722caefa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 20 BPE Merges ---\n",
      "1: ['Ġ', 't']\n",
      "2: ['Ġ', 'a']\n",
      "3: ['i', 'n']\n",
      "4: ['h', 'e']\n",
      "5: ['r', 'e']\n",
      "6: ['o', 'n']\n",
      "7: ['Ġt', 'he']\n",
      "8: ['e', 'r']\n",
      "9: ['Ġ', 's']\n",
      "10: ['Ġ', 'w']\n",
      "11: ['Ġ', 'o']\n",
      "12: ['a', 't']\n",
      "13: ['n', 'd']\n",
      "14: ['Ġ', 'c']\n",
      "15: ['i', 't']\n",
      "16: ['e', 's']\n",
      "17: ['o', 'u']\n",
      "18: ['o', 'r']\n",
      "19: ['i', 's']\n",
      "20: ['Ġ', 'f']\n"
     ]
    }
   ],
   "source": [
    "# 8. Load the saved tokenizer file to inspect its contents\n",
    "with open(tokfile, \"r\") as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "merges = tokenizer_data['model']['merges']\n",
    "\n",
    "n = 20\n",
    "print(f\"\\n--- Top {n} BPE Merges ---\")\n",
    "for i, merge in enumerate(merges[:n]):\n",
    "   print(f\"{i + 1}: {merge!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f740b1a-e573-47ed-bab3-c5dd8ce4e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f\"/Users/sander/Desktop/script_bpe/results/tokenizers/{corpus_name}/n50000/bytes_gpt2.json.gz\") as f:\n",
    "   my_tok = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c8a134-b86b-4d1a-b732-643100e2a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bytes_to_char():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "char_to_byte = {v:k for k,v in bytes_to_char().items()}\n",
    "\n",
    "def demangle_hf_token(token_string: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Converts a token string from the HF tokenizer's vocab or merges\n",
    "    (which uses special characters to represent bytes) back into a raw bytes object.\n",
    "    \n",
    "    Example: demangle_hf_token('âĢ') -> b'\\xe2\\x80'\n",
    "    \"\"\"\n",
    "    byte_values = [char_to_byte[char] for char in token_string]\n",
    "    return bytes(byte_values).decode(errors='backslashreplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3b4f06-256d-4c62-9896-510af8b3afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab = set()\n",
    "hf_vocab = set()\n",
    "\n",
    "my_merges = my_tok['metadata']['tokens'][256:]\n",
    "for i, (hf_merge, my_merge) in enumerate(zip(merges, my_merges)):\n",
    "    hf_froma, hf_fromb = hf_merge\n",
    "    hf_to = demangle_hf_token(hf_froma+hf_fromb)\n",
    "    my_to = my_merge['vocab']\n",
    "    my_vocab.add(my_to)\n",
    "    hf_vocab.add(hf_to)    \n",
    "    if hf_to != my_to and i< 1680:\n",
    "        print(f\"{i}: hf {hf_to!r} \\tmy {my_to!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac7c815-8db8-4d2f-a4d7-73e6758dc470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocab - hf_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b649064b-2f28-4fa5-a600-7d48bf5335d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_vocab - my_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700a3b24-3cc3-46e6-9ef4-67862898ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_merges = my_tok['metadata']['tokens'][256:]\n",
    "for i, (hf_merge, my_merge) in enumerate(zip(merges, my_merges)):\n",
    "    hf_froma, hf_fromb = hf_merge\n",
    "    hf_to = demangle_hf_token(hf_froma+hf_fromb)\n",
    "    my_to = my_merge['vocab']\n",
    "    no_match = (hf_to != my_to)\n",
    "    if my_to not in hf_vocab or hf_to not in my_vocab or no_match:\n",
    "        if my_to not in hf_vocab:\n",
    "            marker = 'm'\n",
    "        elif hf_to not in my_vocab:\n",
    "            marker = 'h'\n",
    "        else:\n",
    "            marker = '?'\n",
    "        print(f\"{marker} {i}: hf {repr(hf_to):20} \\tmy {repr(my_to):20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8752ff29-9885-446b-a4a5-8af9c5a9a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 846440/846440 [00:33<00:00, 25077.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Corpus Compression Statistics ---\n",
      "Total bytes in original text: 299,153,665\n",
      "Total number of tokens generated: 63,629,873\n",
      "Compression Ratio (Bytes/Token): 4.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Initialize counters\n",
    "total_original_bytes = 0\n",
    "total_tokens = 0\n",
    "\n",
    "# 3. Define a function to process a batch of texts\n",
    "def process_batch(batch):\n",
    "    global total_original_bytes, total_tokens\n",
    "    # Calculate the byte size of the original text (in UTF-8)\n",
    "    # The 'text' column is assumed to contain your text data.\n",
    "    text_list = batch[\"text\"]\n",
    "    total_original_bytes += sum(len(text.encode('utf-8')) for text in text_list if text)\n",
    "    \n",
    "    # Tokenize the text and count the number of tokens\n",
    "    # Using batch_encode is much faster than encoding one by one.\n",
    "    encoded_batch = tokenizer.encode_batch(text_list)\n",
    "    total_tokens += sum(len(encoding.ids) for encoding in encoded_batch)\n",
    "\n",
    "dataset.map(\n",
    "    process_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "compression_ratio = total_original_bytes / total_tokens\n",
    "print(\"\\n--- Corpus Compression Statistics ---\")\n",
    "print(f\"Total bytes in original text: {total_original_bytes:,}\")\n",
    "print(f\"Total number of tokens generated: {total_tokens:,}\")\n",
    "print(f\"Compression Ratio (Bytes/Token): {compression_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab4da1-cf22-4377-99e9-e1809bb247eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dc2bd-6964-420e-831f-c15f62ac0e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
