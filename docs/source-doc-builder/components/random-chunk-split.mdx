# RandomChunkSplit

<tokenizerType>PreTokenizer</tokenizerType>

The `RandomChunkSplit` pre-tokenizer splits text into random-length chunks regardless of whitespace boundaries. This enables [BPE](../components/bpe.mdx) models to learn tokens that span across whitespace, which can be particularly useful for recognizing common multi-word expressions as single tokens.

## How it works

Unlike traditional pre-tokenizers like [WhitespaceSplit](../components/whitespace-split.mdx) that split text at whitespace before tokenization, `RandomChunkSplit` randomly segments text into chunks of configurable lengths. The algorithm:

1. Determines a random length between `min_length` and `max_length` for each chunk
2. Splits the input text into chunks of this random length
3. Ensures proper handling of Unicode characters by splitting at character boundaries, not byte positions

This approach allows the BPE algorithm to discover and learn multi-word tokens that occur frequently in the training corpus.

## Example

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import RandomChunkSplit
from tokenizers.trainers import BpeTrainer

# Initialize a tokenizer with BPE model
tokenizer = Tokenizer(BPE())

# Add the RandomChunkSplit pre-tokenizer
tokenizer.pre_tokenizer = RandomChunkSplit(min_length=2, max_length=5) 

# Train on your data
trainer = BpeTrainer(
    vocab_size=25000,
    min_frequency=2,
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
)
tokenizer.train_from_iterator(your_training_data, trainer)

# Now the tokenizer can recognize multi-word expressions
encoded = tokenizer.encode("New York is a city in the United States")
print(encoded.tokens)
```

## Benefits

1. **Multi-word expressions**: Enables learning tokens that span whitespace boundaries
2. **Domain adaptation**: Better handles domain-specific expressions like "New York" or "machine learning"
3. **Tokenization efficiency**: Can reduce sequence lengths by representing common phrases as single tokens
4. **Semantic coherence**: Helps maintain semantic meaning of expressions that should be treated as units

## Parameters

<tokenizerParam name="min_length" type="int" default="1">
    The minimum length (in characters) for each chunk.
</tokenizerParam>

<tokenizerParam name="max_length" type="int" default="5">
    The maximum length (in characters) for each chunk.
</tokenizerParam>

## Usage Recommendations

1. **Chunk Size Tuning**: The optimal chunk size depends on your language and domain:
   - Smaller chunks (1-3): Good for character-rich languages or short words
   - Medium chunks (2-5): Balanced approach for most languages
   - Larger chunks (5-10): Better for capturing longer expressions

2. **Training Impact**:
   - RandomChunkSplit typically requires more training data
   - It may produce larger vocabularies with more diverse tokens
   - Training might be slightly slower due to more complex pattern discovery

3. **Use with other components**:
   - Combines well with normalization steps like lowercasing
   - Works with various BPE trainers and parameters

4. **Evaluation**: Best compared against traditional methods using perplexity or downstream task performance